{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f70785aab70>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.Generator()\n",
    "g.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (1690674131.py, line 21)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[159], line 21\u001b[0;36m\u001b[0m\n\u001b[0;31m    x = self.\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class MyTransformer:\n",
    "    pass\n",
    "\n",
    "\n",
    "class MyEncoderLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        max_seq: int,\n",
    "        hidden: int,\n",
    "        drop_prob: float,\n",
    "        eps: float = 1e-5\n",
    "    ) -> None:\n",
    "        # self.positional_encoding = MyPositionalEncoding(d_model, max_seq)\n",
    "        self.attention_layer = MyMultiheadAttention(d_model, num_heads)\n",
    "        self.layer_norm = MyLayerNorm(d_model, eps=1e-5)\n",
    "        self.ffn = MyFNN(d_model, hidden, drop_prob, eps)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        x = self.\n",
    "\n",
    "\n",
    "class MyEncoder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MyDecoderLayer:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class MyDecoder:\n",
    "    pass\n",
    "\n",
    "\n",
    "class MySentenceEmbedding:\n",
    "    \"\"\"\n",
    "    1. Convert word into indices\n",
    "        - start token\n",
    "        - end token\n",
    "        - padding token\n",
    "    2. create embeddings\n",
    "    3. \n",
    "    \"\"\"\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "    \n",
    "    \n",
    "\n",
    "class MySingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.qkv_layer = nn.Linear(self.d_model, self.d_model * 3)\n",
    "        self.out_layer = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        attention = softmax(Q @ K.T/√dim) @ V\n",
    "        \"\"\"\n",
    "        print(f\"x.size: {x.size()}\")\n",
    "        batch, max_seq = x.size()[:2]\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        scaled_dot = q @ k.transpose(-1, -2) / math.sqrt(q.size()[-1])\n",
    "        attention = F.softmax(scaled_dot @ v, dim=-1)\n",
    "        print(f\"attention size: {attention.size()}\")\n",
    "        out = self.out_layer(attention)\n",
    "        print(f\"out size: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyMultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"Must be divisible\"\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        attention = softmax(Q @ K.T/√dim) @ V\n",
    "        \"\"\"\n",
    "        batch, seq, _ = x.size()\n",
    "        print(f\"x.size: {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch, seq, self.num_heads, self.head_dim * 3)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q.size: {qkv.size()}\")\n",
    "        attention = F.softmax(\n",
    "            q @ k.transpose(-1, -2) / math.sqrt(q.size()[-1]) @ v, dim=-1\n",
    "        )\n",
    "        print(f\"attention.size: {attention.size()}\")\n",
    "        attention = attention.reshape(batch, seq, self.num_heads * self.head_dim)\n",
    "        print(f\"attention.size: {attention.size()}\")\n",
    "        out = self.out_layer(attention)\n",
    "        print(f\"out.size: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq: int) -> None:\n",
    "        self.d_model = d_model\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def forward(self) -> torch.tensor:\n",
    "        even_i = torch.arange(0, self.max_seq, 2)\n",
    "        odd_i = torch.arange(1, self.max_seq, 2)\n",
    "        position_half = torch.arange(\n",
    "            start=0, end=self.max_seq // 2, step=1, dtype=torch.float32\n",
    "        )\n",
    "        position = torch.repeat_interleave(position_half, 2).reshape(self.max_seq, 1)\n",
    "        print(f\"position: {position}, shape: {position.shape}\")\n",
    "        pe_even = torch.sin(position / torch.pow(10000, (2 * even_i / self.d_model)))\n",
    "        pe_odd = torch.sin(position / torch.pow(10000, (2 * odd_i / self.d_model)))\n",
    "        pe = torch.stack((pe_even, pe_odd), dim=-1).flatten(start_dim=1)\n",
    "        print(f\"pe: {pe}, shape: {pe.shape}\")\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, ebed_dim: int, eps: int = 1e-5) -> None:\n",
    "        self.ebed_dim = ebed_dim\n",
    "        self.gamma = nn.Parameter(nn.ones(ebed_dim, dtype=torch.float32))\n",
    "        self.beta = nn.Parameter(nn.zeros(ebed_dim, dtype=torch.float32))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        mean = torch.mean(x, dim=(-1, -2), keepdim=True)\n",
    "        var = torch.var(x, dim=(-1, -2), keepdim=True)\n",
    "        std = (var + self.eps) ** 0.5\n",
    "        out = (x - mean) / std * self.gamma + self.beta\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyFNN:\n",
    "    def __init__(self, ebed_dim: int, hidden: int, drop_prob: float = 0.2) -> None:\n",
    "        self.ebed_dim = ebed_dim\n",
    "        self.linear1 = nn.Linear(ebed_dim, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, ebed_dim)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu\n",
    "        out = self.linear2(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyEncoderLayer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model: int,\n",
    "        num_heads: int,\n",
    "        hidden: int,\n",
    "        drop_prob: float,\n",
    "        eps: float = 1e-5\n",
    "    ) -> None:\n",
    "        # self.positional_encoding = MyPositionalEncoding(d_model, max_seq)\n",
    "        self.attention_layer = MyMultiheadAttention(d_model, num_heads)\n",
    "        self.layer_norm1 = MyLayerNorm(d_model, eps=1e-5)\n",
    "        self.layer_norm2 = MyLayerNorm(d_model, eps=1e-5)\n",
    "        self.ffn = MyFNN(d_model, hidden, drop_prob, eps)\n",
    "        self.dropout1 = nn.Dropout(p=drop_prob)\n",
    "        self.dropout2 = nn.Dropout(p=drop_prob)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        residual = x.clone()\n",
    "        x = self.attention_layer(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.layer_norm1(x + residual)\n",
    "        residual = x.clone()\n",
    "        x = self.ffn(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.layer_norm2(x + residual)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MyEncoder:\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "\n",
    "class MyDecoderLayer:\n",
    "    pass\n",
    "\n",
    "\n",
    "\n",
    "class MyDecoder:\n",
    "    pass\n",
    "\n",
    "\n",
    "class SingleHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.qkv_layer = nn.Linear(self.d_model, self.d_model * 3)\n",
    "        self.out_layer = nn.Linear(self.d_model, self.d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        attention = softmax(Q @ K.T/√dim) @ V\n",
    "        \"\"\"\n",
    "        print(f\"x.size: {x.size()}\")\n",
    "        batch, max_seq = x.size()[:2]\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        q, k, v = torch.chunk(qkv, 3, dim=-1)\n",
    "        scaled_dot = q @ k.transpose(-1, -2) / math.sqrt(q.size()[-1])\n",
    "        attention = F.softmax(scaled_dot @ v, dim=-1)\n",
    "        print(f\"attention size: {attention.size()}\")\n",
    "        out = self.out_layer(attention)\n",
    "        print(f\"out size: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyMultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        assert d_model % num_heads == 0, \"Must be divisible\"\n",
    "        self.head_dim = d_model // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.qkv_layer = nn.Linear(d_model, d_model * 3)\n",
    "        self.out_layer = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        \"\"\"\n",
    "        attention = softmax(Q @ K.T/√dim) @ V\n",
    "        \"\"\"\n",
    "        batch, seq, _ = x.size()\n",
    "        print(f\"x.size: {x.size()}\")\n",
    "        qkv = self.qkv_layer(x)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.reshape(batch, seq, self.num_heads, self.head_dim * 3)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        qkv = qkv.permute(0, 2, 1, 3)\n",
    "        print(f\"qkv.size: {qkv.size()}\")\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        print(f\"q.size: {qkv.size()}\")\n",
    "        attention = F.softmax(\n",
    "            q @ k.transpose(-1, -2) / math.sqrt(q.size()[-1]) @ v, dim=-1\n",
    "        )\n",
    "        print(f\"attention.size: {attention.size()}\")\n",
    "        attention = attention.reshape(batch, seq, self.num_heads * self.head_dim)\n",
    "        print(f\"attention.size: {attention.size()}\")\n",
    "        out = self.out_layer(attention)\n",
    "        print(f\"out.size: {out.size()}\")\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model: int, max_seq: int) -> None:\n",
    "        self.d_model = d_model\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def forward(self) -> torch.tensor:\n",
    "        even_i = torch.arange(0, self.max_seq, 2)\n",
    "        odd_i = torch.arange(1, self.max_seq, 2)\n",
    "        position_half = torch.arange(\n",
    "            start=0, end=self.max_seq // 2, step=1, dtype=torch.float32\n",
    "        )\n",
    "        position = torch.repeat_interleave(position_half, 2).reshape(self.max_seq, 1)\n",
    "        print(f\"position: {position}, shape: {position.shape}\")\n",
    "        pe_even = torch.sin(position / torch.pow(10000, (2 * even_i / self.d_model)))\n",
    "        pe_odd = torch.sin(position / torch.pow(10000, (2 * odd_i / self.d_model)))\n",
    "        pe = torch.stack((pe_even, pe_odd), dim=-1).flatten(start_dim=1)\n",
    "        print(f\"pe: {pe}, shape: {pe.shape}\")\n",
    "        return pe\n",
    "\n",
    "\n",
    "class MyLayerNorm(nn.Module):\n",
    "    def __init__(self, ebed_dim: int, eps: int = 1e-5) -> None:\n",
    "        self.ebed_dim = ebed_dim\n",
    "        self.gamma = nn.Parameter(nn.ones(ebed_dim, dtype=torch.float32))\n",
    "        self.beta = nn.Parameter(nn.zeros(ebed_dim, dtype=torch.float32))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        mean = torch.mean(x, dim=(-1, -2), keepdim=True)\n",
    "        var = torch.var(x, dim=(-1, -2), keepdim=True)\n",
    "        std = (var + self.eps) ** 0.5\n",
    "        out = (x - mean) / std * self.gamma + self.beta\n",
    "        return out\n",
    "\n",
    "\n",
    "class MyFNN:\n",
    "    def __init__(self, ebed_dim: int, hidden: int, drop_prob: float = 0.2) -> None:\n",
    "        self.ebed_dim = ebed_dim\n",
    "        self.linear1 = nn.Linear(ebed_dim, hidden)\n",
    "        self.linear2 = nn.Linear(hidden, ebed_dim)\n",
    "        self.dropout = nn.Dropout(p=drop_prob)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x) -> torch.tensor:\n",
    "        x = self.linear1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu\n",
    "        out = self.linear2(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([8, 16])"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.size()[-2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0524]],\n",
       "\n",
       "        [[-0.0010]]])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(dim=(-1, -2), keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.1258e+00, -1.1524e+00, -2.5058e-01, -4.3388e-01,  8.4871e-01,\n",
       "           6.9201e-01, -3.1601e-01, -2.1152e+00,  3.2227e-01, -1.2633e+00,\n",
       "           3.4998e-01,  3.0813e-01,  1.1984e-01,  1.2377e+00,  1.1168e+00,\n",
       "          -2.4728e-01],\n",
       "         [-1.3527e+00, -1.6959e+00,  5.6665e-01,  7.9351e-01,  5.9884e-01,\n",
       "          -1.5551e+00, -3.4136e-01,  1.8530e+00,  7.5019e-01, -5.8550e-01,\n",
       "          -1.7340e-01,  1.8348e-01,  1.3894e+00,  1.5863e+00,  9.4630e-01,\n",
       "          -8.4368e-01],\n",
       "         [-6.1358e-01,  3.1593e-02, -4.9268e-01,  2.4841e-01,  4.3970e-01,\n",
       "           1.1241e-01,  6.4079e-01,  4.4116e-01, -1.0231e-01,  7.9244e-01,\n",
       "          -2.8967e-01,  5.2507e-02,  5.2286e-01,  2.3022e+00, -1.4689e+00,\n",
       "          -1.5867e+00],\n",
       "         [-6.7309e-01,  8.7283e-01,  1.0554e+00,  1.7784e-01, -2.3034e-01,\n",
       "          -3.9175e-01,  5.4329e-01, -3.9516e-01, -4.4622e-01,  7.4402e-01,\n",
       "           1.5210e+00,  3.4105e+00, -1.5312e+00, -1.2341e+00,  1.8197e+00,\n",
       "          -5.5153e-01],\n",
       "         [-5.6925e-01,  9.1997e-01,  1.1108e+00,  1.2899e+00, -1.4782e+00,\n",
       "           2.5672e+00, -4.7312e-01,  3.3555e-01, -1.6293e+00, -5.4974e-01,\n",
       "          -4.7983e-01, -4.9968e-01, -1.0670e+00,  1.1149e+00, -1.4067e-01,\n",
       "           8.0575e-01],\n",
       "         [-9.3348e-02,  6.8705e-01, -8.3832e-01,  8.9182e-04,  8.4189e-01,\n",
       "          -4.0003e-01,  1.0395e+00,  3.5815e-01, -2.4600e-01,  2.3025e+00,\n",
       "          -1.8817e+00, -4.9727e-02, -1.0450e+00, -9.5650e-01,  3.3532e-02,\n",
       "           7.1009e-01],\n",
       "         [ 1.6459e+00, -1.3602e+00,  3.4457e-01,  5.1987e-01, -2.6133e+00,\n",
       "          -1.6965e+00, -2.2824e-01,  2.7995e-01,  2.4693e-01,  7.6887e-02,\n",
       "           3.3801e-01,  4.5440e-01,  4.5694e-01, -8.6537e-01,  7.8131e-01,\n",
       "          -9.2679e-01],\n",
       "         [-2.1883e-01, -2.4351e+00, -7.2915e-02, -3.3986e-02,  9.6252e-01,\n",
       "           3.4917e-01, -9.2146e-01, -5.6195e-02, -6.2270e-01, -4.6372e-01,\n",
       "           1.9218e+00, -4.0255e-01,  1.2390e-01,  1.1648e+00,  9.2337e-01,\n",
       "           1.3873e+00]],\n",
       "\n",
       "        [[-8.8338e-01, -4.1891e-01, -8.0483e-01,  5.6561e-01,  6.1036e-01,\n",
       "           4.6688e-01,  1.9507e+00, -1.0631e+00, -7.7326e-02,  1.1640e-01,\n",
       "          -5.9399e-01, -1.2439e+00, -1.0209e-01, -1.0335e+00, -3.1264e-01,\n",
       "           2.4579e-01],\n",
       "         [-2.5964e-01,  1.1834e-01,  2.4396e-01,  1.1646e+00,  2.8858e-01,\n",
       "           3.8660e-01, -2.0106e-01, -1.1793e-01,  1.9220e-01, -7.7216e-01,\n",
       "          -1.9003e+00,  1.3068e-01, -7.0429e-01,  3.1472e-01,  1.5739e-01,\n",
       "           3.8536e-01],\n",
       "         [ 9.6715e-01, -9.9108e-01,  3.0161e-01, -1.0732e-01,  9.9846e-01,\n",
       "          -4.9871e-01,  7.6111e-01,  6.1830e-01,  3.1405e-01,  2.1333e-01,\n",
       "          -1.2005e-01,  3.6046e-01, -3.1403e-01, -1.0787e+00,  2.4081e-01,\n",
       "          -1.3962e+00],\n",
       "         [-6.6144e-02, -3.5836e-01, -1.5616e+00, -3.5464e-01,  1.0811e+00,\n",
       "           1.3148e-01,  1.5735e+00,  7.8143e-01, -1.0787e+00, -7.2091e-01,\n",
       "           1.4708e+00,  2.7564e-01,  6.6678e-01, -9.9439e-01, -1.1894e+00,\n",
       "          -1.1959e+00],\n",
       "         [-5.5963e-01,  5.3347e-01,  4.0689e-01,  3.9459e-01,  1.7151e-01,\n",
       "           8.7604e-01, -2.8709e-01,  1.0216e+00, -7.4395e-02, -1.0922e+00,\n",
       "           3.9203e-01,  5.9453e-01,  6.6227e-01, -1.2063e+00,  6.0744e-01,\n",
       "          -5.4716e-01],\n",
       "         [ 1.1711e+00,  9.7496e-02,  9.6337e-01,  8.4032e-01, -1.2537e+00,\n",
       "           9.8684e-01, -4.9466e-01, -1.2830e+00,  9.5522e-01,  1.2836e+00,\n",
       "          -6.6586e-01,  5.6513e-01,  2.8770e-01, -3.3375e-02, -1.0619e+00,\n",
       "          -1.1443e-01],\n",
       "         [-3.4334e-01,  1.5713e+00,  1.9161e-01,  3.7994e-01, -1.4476e-01,\n",
       "           6.3762e-01, -2.8129e-01, -1.3299e+00, -1.4201e-01, -5.3415e-01,\n",
       "          -5.2338e-01,  8.6150e-01, -8.8696e-01,  8.3877e-01,  1.1529e+00,\n",
       "          -1.7611e+00],\n",
       "         [-1.4777e+00, -1.7557e+00,  7.6166e-02, -1.0786e+00,  1.4403e+00,\n",
       "          -1.1059e-01,  5.7686e-01, -1.6917e-01, -6.4025e-02,  1.0384e+00,\n",
       "           9.0682e-01, -4.7551e-01, -8.7074e-01,  1.4474e-01,  1.9029e+00,\n",
       "           3.9040e-01]]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq = 8\n",
    "batch = 2\n",
    "d_model = 16\n",
    "x = torch.randn(batch, max_seq, d_model, generator=g)\n",
    "print(x.size())\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.size: torch.Size([2, 8, 16])\n",
      "qkv.size: torch.Size([2, 8, 48])\n",
      "qkv.size: torch.Size([2, 8, 4, 12])\n",
      "qkv.size: torch.Size([2, 4, 8, 12])\n",
      "q.size: torch.Size([2, 4, 8, 12])\n",
      "attention.size: torch.Size([2, 4, 8, 4])\n",
      "attention.size: torch.Size([2, 8, 16])\n",
      "out.size: torch.Size([2, 8, 16])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.6080e-01,  3.8284e-01, -4.4325e-03, -1.3599e-01, -1.8348e-02,\n",
       "          -2.1855e-01,  5.3361e-02,  1.6159e-01, -1.6878e-01, -1.8494e-01,\n",
       "          -3.4242e-01, -9.3192e-02,  2.2041e-01, -1.2789e-01,  1.6106e-01,\n",
       "           3.4265e-02],\n",
       "         [-1.9514e-01,  4.4145e-01,  1.0494e-01, -6.8306e-02,  6.4535e-02,\n",
       "          -2.2076e-02,  7.5763e-02,  2.1320e-01, -2.5232e-02, -3.3109e-01,\n",
       "          -3.2964e-01, -1.4295e-01,  1.5792e-01, -2.8889e-01,  1.0140e-02,\n",
       "          -1.8596e-01],\n",
       "         [-3.9832e-02,  5.5733e-01,  3.1923e-01,  3.6236e-02, -1.0058e-01,\n",
       "          -4.2197e-02,  1.3679e-01,  2.3495e-01,  1.7310e-02, -9.9014e-02,\n",
       "          -1.8496e-01, -3.4619e-01,  2.3857e-01, -2.3925e-01,  1.2257e-01,\n",
       "          -8.6098e-02],\n",
       "         [-2.0804e-01,  3.8881e-01,  4.2467e-02, -7.2547e-02, -1.2706e-02,\n",
       "          -1.7483e-01,  5.8801e-02,  1.4549e-01, -1.3260e-01, -1.9473e-01,\n",
       "          -3.2912e-01, -5.0149e-02,  2.1737e-01, -1.9278e-01,  1.2767e-01,\n",
       "          -2.5657e-02],\n",
       "         [-1.2296e-01,  5.3169e-01,  2.1301e-01, -2.1559e-02, -6.5811e-02,\n",
       "          -5.6496e-03,  1.7616e-01, -1.0190e-02, -2.1323e-02, -3.3398e-01,\n",
       "          -1.7190e-01, -1.3679e-01,  2.1710e-01, -3.0378e-01, -3.7122e-02,\n",
       "          -2.1736e-01],\n",
       "         [-2.6608e-01,  3.8402e-01,  5.8302e-02, -5.8043e-02,  1.1913e-01,\n",
       "          -1.1069e-02,  7.3756e-02,  1.9477e-01, -1.4447e-01, -4.0964e-01,\n",
       "          -3.8674e-01, -8.7492e-02,  2.0111e-01, -2.7613e-01,  3.1372e-02,\n",
       "          -2.6094e-01],\n",
       "         [-1.3416e-01,  4.1896e-01,  1.6350e-01, -4.7909e-03,  1.2285e-02,\n",
       "          -9.5429e-02,  3.1731e-02,  9.9061e-02, -9.3149e-02, -2.8487e-01,\n",
       "          -3.0496e-01, -1.1039e-01,  2.1231e-01, -2.2645e-01,  1.1254e-01,\n",
       "          -8.8978e-02],\n",
       "         [-1.0515e-01,  4.7987e-01,  2.4032e-01, -4.0658e-02, -4.2271e-02,\n",
       "          -1.4720e-01,  2.3143e-02,  9.0551e-02,  1.7839e-02, -3.1108e-01,\n",
       "          -2.7076e-01, -1.7816e-01,  1.4820e-01, -2.6364e-01,  1.4065e-02,\n",
       "          -1.1783e-01]],\n",
       "\n",
       "        [[-2.4144e-01,  4.0230e-01,  1.0886e-01, -7.8133e-03,  2.3412e-02,\n",
       "          -1.6364e-02,  1.7678e-01,  4.6873e-02, -1.6459e-01, -3.3838e-01,\n",
       "          -2.6813e-01, -4.8352e-02,  2.6765e-01, -2.4879e-01,  2.0389e-02,\n",
       "          -2.5587e-01],\n",
       "         [-1.1924e-01,  4.2094e-01,  1.3654e-01, -6.7619e-02, -2.0497e-02,\n",
       "          -1.0856e-01,  7.8367e-02,  8.5085e-02,  7.8714e-03, -2.4096e-01,\n",
       "          -3.1837e-01, -1.1342e-01,  1.8471e-01, -2.5074e-01,  4.8873e-02,\n",
       "          -6.9353e-02],\n",
       "         [-1.6787e-01,  3.7657e-01,  1.1492e-01, -5.5800e-02,  6.2102e-02,\n",
       "          -6.7530e-02,  1.2442e-01,  1.7596e-01, -6.8314e-02, -3.1176e-01,\n",
       "          -2.7822e-01, -9.1051e-02,  1.9965e-01, -2.5583e-01,  3.1899e-02,\n",
       "          -1.6561e-01],\n",
       "         [-1.9449e-01,  3.8675e-01,  1.0188e-01, -3.2021e-02,  2.2034e-02,\n",
       "          -1.0394e-01,  4.2476e-05,  9.8905e-02, -4.3177e-02, -2.4260e-01,\n",
       "          -4.0819e-01, -7.7937e-02,  1.8815e-01, -2.0632e-01,  8.3326e-02,\n",
       "          -6.1167e-02],\n",
       "         [-1.9913e-01,  3.9304e-01,  1.0493e-01, -7.7932e-03, -9.9939e-03,\n",
       "          -1.1913e-01,  1.6534e-01,  9.4402e-02, -1.2034e-01, -2.0245e-01,\n",
       "          -3.3191e-01, -1.3445e-01,  1.8995e-01, -2.3926e-01,  8.5507e-02,\n",
       "          -1.4894e-01],\n",
       "         [-1.3146e-01,  4.0695e-01,  1.3115e-01, -8.6277e-02,  1.5207e-02,\n",
       "          -9.9172e-02,  5.1269e-02,  1.3686e-01,  1.8577e-02, -2.9089e-01,\n",
       "          -3.0377e-01, -9.5734e-02,  1.7526e-01, -2.5938e-01,  2.5036e-02,\n",
       "          -9.1436e-02],\n",
       "         [-1.8955e-01,  3.0209e-01,  9.5169e-02, -4.9251e-02,  5.9518e-02,\n",
       "          -2.5880e-02,  1.0545e-01,  8.2775e-02, -4.0781e-02, -3.1136e-01,\n",
       "          -3.2458e-01, -1.3241e-02,  2.5031e-01, -2.4350e-01,  2.8583e-02,\n",
       "          -1.7860e-01],\n",
       "         [-1.3666e-01,  3.6540e-01,  1.3586e-01, -3.9549e-02,  3.7167e-02,\n",
       "          -7.0991e-02,  4.8615e-02,  8.1720e-02,  1.5996e-02, -2.6400e-01,\n",
       "          -3.3934e-01, -5.1673e-02,  2.0207e-01, -2.1539e-01,  3.4705e-02,\n",
       "          -6.5925e-02]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mht = MyMultiheadAttention(d_model=d_model, num_heads=4)\n",
    "mht.forward(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "position: tensor([[0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [2.],\n",
      "        [2.],\n",
      "        [3.],\n",
      "        [3.]]), shape: torch.Size([8, 1])\n",
      "pe: tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00],\n",
      "        [8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
      "         1.0000e-03, 3.1623e-04],\n",
      "        [8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
      "         1.0000e-03, 3.1623e-04],\n",
      "        [9.0930e-01, 5.9113e-01, 1.9867e-01, 6.3203e-02, 1.9999e-02, 6.3245e-03,\n",
      "         2.0000e-03, 6.3246e-04],\n",
      "        [9.0930e-01, 5.9113e-01, 1.9867e-01, 6.3203e-02, 1.9999e-02, 6.3245e-03,\n",
      "         2.0000e-03, 6.3246e-04],\n",
      "        [1.4112e-01, 8.1265e-01, 2.9552e-01, 9.4726e-02, 2.9995e-02, 9.4867e-03,\n",
      "         3.0000e-03, 9.4868e-04],\n",
      "        [1.4112e-01, 8.1265e-01, 2.9552e-01, 9.4726e-02, 2.9995e-02, 9.4867e-03,\n",
      "         3.0000e-03, 9.4868e-04]]), shape: torch.Size([8, 8])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
       "         0.0000e+00, 0.0000e+00],\n",
       "        [8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
       "         1.0000e-03, 3.1623e-04],\n",
       "        [8.4147e-01, 3.1098e-01, 9.9833e-02, 3.1618e-02, 9.9998e-03, 3.1623e-03,\n",
       "         1.0000e-03, 3.1623e-04],\n",
       "        [9.0930e-01, 5.9113e-01, 1.9867e-01, 6.3203e-02, 1.9999e-02, 6.3245e-03,\n",
       "         2.0000e-03, 6.3246e-04],\n",
       "        [9.0930e-01, 5.9113e-01, 1.9867e-01, 6.3203e-02, 1.9999e-02, 6.3245e-03,\n",
       "         2.0000e-03, 6.3246e-04],\n",
       "        [1.4112e-01, 8.1265e-01, 2.9552e-01, 9.4726e-02, 2.9995e-02, 9.4867e-03,\n",
       "         3.0000e-03, 9.4868e-04],\n",
       "        [1.4112e-01, 8.1265e-01, 2.9552e-01, 9.4726e-02, 2.9995e-02, 9.4867e-03,\n",
       "         3.0000e-03, 9.4868e-04]])"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mype = MyPositionalEncoding(d_model=d_model, max_seq=max_seq)\n",
    "mype.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8835, -0.5635, -0.4923],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000],\n",
       "         [ 0.8835, -0.5635, -0.4923],\n",
       "         [ 0.8835, -0.5635, -0.4923],\n",
       "         [ 0.8835, -0.5635, -0.4923],\n",
       "         [-0.4962, -0.2699,  1.3900]]], grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding = nn.Embedding(100, 3, padding_idx=0)\n",
    "input_x = torch.tensor([[0,0,0,1,0,0,1,1,1,2]])\n",
    "embedding(input_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
